name: macOS
on:
  pull_request:
  push:
    branches:
      - main
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build:
    runs-on: macos-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v5
      - name: Install Go
        uses: actions/setup-go@v6
        with:
          go-version: 'stable'
      - name: Get latest llama.cpp version
        id: llama-version
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          VERSION=$(gh api repos/ggml-org/llama.cpp/releases/latest --jq '.tag_name')
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Latest llama.cpp version: $VERSION"
      - name: Install yzma command
        run: |
          cd cmd/yzma
          go install .
      - name: Install llama.cpp binaries
        run: |
          set -e
          yzma install -lib $GITHUB_WORKSPACE/lib -version ${{ steps.llama-version.outputs.version }} -p metal
      - name: Add llama.cpp libs to path
        run: |
            echo "YZMA_LIB=$GITHUB_WORKSPACE/lib" >> "$GITHUB_ENV"
            echo "LD_LIBRARY_PATH=$GITHUB_WORKSPACE/lib" >> "$GITHUB_ENV"
      - name: Cache test models
        id: cache-models
        uses: actions/cache@v4
        with:
          path: models
          key: ${{ runner.os }}-models-v3
      - name: Download test models
        if: steps.cache-models.outputs.cache-hit != 'true'
        run: |
            mkdir -p ./models
            curl -Lo ./models/SmolLM-135M.Q2_K.gguf https://huggingface.co/QuantFactory/SmolLM-135M-GGUF/resolve/main/SmolLM-135M.Q2_K.gguf
            curl -Lo ./models/SmolVLM-256M-Instruct-Q8_0.gguf https://huggingface.co/ggml-org/SmolVLM-256M-Instruct-GGUF/resolve/main/SmolVLM-256M-Instruct-Q8_0.gguf
            curl -Lo ./models/mmproj-SmolVLM-256M-Instruct-Q8_0.gguf https://huggingface.co/ggml-org/SmolVLM-256M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-256M-Instruct-Q8_0.gguf
            curl -Lo ./models/ggml-model-f16.gguf https://huggingface.co/ggml-org/models-moved/resolve/main/jina-reranker-v1-tiny-en/ggml-model-f16.gguf
            curl -Lo ./models/t5base-encoder-q4_0.gguf https://huggingface.co/callgg/t5-base-encoder-f32/resolve/main/t5base-encoder-q4_0.gguf
            curl -Lo ./models/Gemma2-Base-F32.gguf https://huggingface.co/deadprogram/yzma-tests/resolve/main/Gemma2-Base-F32.gguf
            curl -Lo ./models/Gemma2-Lora-F32-LoRA.gguf https://huggingface.co/deadprogram/yzma-tests/resolve/main/Gemma2-Lora-F32-LoRA.gguf
      - name: Run unit tests
        run: |
          export YZMA_TEST_MODEL=$GITHUB_WORKSPACE/models/SmolLM-135M.Q2_K.gguf
          export YZMA_TEST_MMMODEL=$GITHUB_WORKSPACE/models/SmolVLM-256M-Instruct-Q8_0.gguf
          export YZMA_TEST_MMPROJ=$GITHUB_WORKSPACE/models/mmproj-SmolVLM-256M-Instruct-Q8_0.gguf
          export YZMA_TEST_QUANTIZE_MODEL=$GITHUB_WORKSPACE/models/ggml-model-f16.gguf
          export YZMA_TEST_ENCODER_MODEL=$GITHUB_WORKSPACE/models/t5base-encoder-q4_0.gguf
          export YZMA_TEST_LORA_MODEL=$GITHUB_WORKSPACE/models/Gemma2-Base-F32.gguf
          export YZMA_TEST_LORA_ADAPTER=$GITHUB_WORKSPACE/models/Gemma2-Lora-F32-LoRA.gguf
          CGO_ENABLED=0 go test -v ./...
      - name: Run inference test
        run: go run ./examples/hello
      - name: Run embedding test
        run: go run ./examples/embeddings -model $GITHUB_WORKSPACE/models/SmolLM-135M.Q2_K.gguf -p "Hello World"
