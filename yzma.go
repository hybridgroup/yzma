// yzma lets you write Go applications that directly integrate llama.cpp
// for fully local inference using hardware acceleration.
//
// Run the latest Vision Language Models and Large Language Models
// on Linux, on macOS, and on Windows.
// Use hardware acceleration such as CUDA, Metal, and Vulkan.
//
// yzma uses the purego (https://github.com/ebitengine/purego) and
// ffi (https://github.com/JupiterRider/ffi) packages so CGo is not needed.
//
// This also means that yzma works with the newest llama.cpp releases.
package yzma
